# -*- coding: utf-8 -*-
"""ProjectML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Z-V90SMplhDW10Zd-75-mbamwUR3E0E

**IMPORT LIBRARIES**
"""

import os
import zipfile
import numpy as np
import pandas as pd
from PIL import Image
import torch.nn as nn
from collections import OrderedDict
import torchvision.transforms as transforms
from torchvision.datasets.vision import VisionDataset
import matplotlib.pyplot as plt
import torch.optim as optim
from torch.utils.data import Subset, DataLoader
from torch.backends import cudnn
import torch
import math
import copy

"""**CUSTOM FUNCTIONS**"""

def down_data(user, key, filename):
  os.environ['KAGGLE_USERNAME'] = user # username from the json file
  os.environ['KAGGLE_KEY'] = key # key from the json file
  !kaggle competitions download -c facial-keypoints-detection # api copied from kaggle
  os.chdir('/content')
  for file in os.listdir():
    if (file.endswith('.zip')):
      zip_ref = zipfile.ZipFile(file, 'r')
      zip_ref.extractall()
      zip_ref.close()
      os.remove(file)
  for file in os.listdir():
    print (file)
  return pd.read_csv(filename)

def getimage(immagine):
  imag = []
  
  img = immagine.str.split(' ')
  img = ['0' if x == '' else x for x in img]
  imag.append(img)
  imag_f = np.array(imag,dtype = 'double')
  imag_f = imag_f.reshape(96,96)
  return imag_f

def pil_loader(path):
  image = Image.fromarray(path)
  return image

def make_dataset(filename):
  items = []
  raw_data = down_data("gjcode", "d61a71698d7871c82e569e8eb9be3c10", filename)
  raw_data.isnull().any().value_counts()
  raw_data.fillna(method="ffill", inplace=True)
  count = 0
  for item in range(len(raw_data)):
    image = None
    labels = []
    row = raw_data.loc[[item]]
    labels.append(row.left_eye_center_x.values[0])
    labels.append(row.left_eye_center_y.values[0])
    labels.append(row.right_eye_center_x.values[0])
    labels.append(row.right_eye_center_y.values[0])
    labels.append(row.left_eye_inner_corner_x.values[0])
    labels.append(row.left_eye_inner_corner_y.values[0])
    labels.append(row.left_eye_outer_corner_x.values[0])
    labels.append(row.left_eye_outer_corner_y.values[0])
    labels.append(row.right_eye_inner_corner_x.values[0])
    labels.append(row.right_eye_inner_corner_y.values[0])
    labels.append(row.right_eye_outer_corner_x.values[0])
    labels.append(row.right_eye_outer_corner_y.values[0])
    labels.append(row.left_eyebrow_inner_end_x.values[0])
    labels.append(row.left_eyebrow_inner_end_y.values[0])
    labels.append(row.left_eyebrow_outer_end_x.values[0])
    labels.append(row.left_eyebrow_outer_end_y.values[0])
    labels.append(row.right_eyebrow_inner_end_x.values[0])
    labels.append(row.right_eyebrow_inner_end_y.values[0])
    labels.append(row.right_eyebrow_outer_end_x.values[0])
    labels.append(row.right_eyebrow_outer_end_y.values[0])
    labels.append(row.nose_tip_x.values[0])
    labels.append(row.nose_tip_y.values[0])
    labels.append(row.mouth_left_corner_x.values[0])
    labels.append(row.mouth_left_corner_y.values[0])
    labels.append(row.mouth_right_corner_x.values[0])
    labels.append(row.mouth_right_corner_y.values[0])
    labels.append(row.mouth_center_top_lip_x.values[0])
    labels.append(row.mouth_center_top_lip_y.values[0])
    labels.append(row.mouth_center_bottom_lip_x.values[0])
    labels.append(row.mouth_center_bottom_lip_y.values[0])
    image = getimage(row.Image) # returns a pandas.core.arrays.numpy_.PandasArray, todo: convertire in immagine
    item = (image, np.asarray(labels))
    items.append(item)
    if count % 1217 == 0:
      print("labels from make_dataset:", labels)
    count += 1
  return items

def train(train_dataloader, network):
  res = 0.0
  count = 0.0
  for images, labels in train_dataloader:
    images = images.to(DEVICE)
    labels = labels.to(DEVICE)
    network.train() 
    optimizer.zero_grad() 
    outputs = network(images)
    loss = criterion(outputs, labels)
    res += loss.item()
    loss.backward()  
    optimizer.step() 
    count += 1
  return res / float(count)

def validate(validate_dataloader, network):
  res = 0.0
  count = 0.0
  network.train(False)
  for images, labels in validate_dataloader:
    images = images.to(DEVICE)
    labels = labels.to(DEVICE)
    outputs = network(images)
    loss = criterion(outputs, labels)
    res += loss.item()
    count += 1
  return res / float(count)

"""**CUSTOM CLASSESS**"""

class ResultItem():

  def __init__(self, epoch, batch, lr, tr_loss, vd_loss):
    self.epoch = epoch
    self.batch = batch
    self.lr = lr
    self.tr_loss = tr_loss
    self.vd_loss = vd_loss

class LeNet5(nn.Module):
    """
    Input - 1x32x32
    C1 - 6@28x28 (5x5 kernel)
    tanh
    S2 - 6@14x14 (2x2 kernel, stride 2) Subsampling
    C3 - 16@10x10 (5x5 kernel, complicated shit)
    tanh
    S4 - 16@5x5 (2x2 kernel, stride 2) Subsampling
    C5 - 120@1x1 (5x5 kernel)
    F6 - 84
    tanh
    F7 - 30 (Output)
    """
    def __init__(self):
        super(LeNet5, self).__init__()

        self.convnet = nn.Sequential(OrderedDict([
            ('c1', nn.Conv2d(1, 6, kernel_size=(5, 5))), # 6 -> 32
            ('relu1', nn.ReLU()),
            ('s2', nn.MaxPool2d(kernel_size=(2, 2), stride=2)),
            ('c3', nn.Conv2d(6, 32, kernel_size=(5, 5))), # 6 -> 32, 16 -> 64
            ('relu3', nn.ReLU()),
            ('s4', nn.MaxPool2d(kernel_size=(2, 2), stride=2)),
            ('c5', nn.Conv2d(32, 128, kernel_size=(2, 2))),
            ('relu5', nn.ReLU()),
            ('s6', nn.MaxPool2d(kernel_size=(2, 2), stride=2)),
            ('c7', nn.Conv2d(128, 256, kernel_size=(5, 5))),
            ('relu7', nn.ReLU()),
            ('s8', nn.MaxPool2d(kernel_size=(2, 2), stride=2)),
            ('c9', nn.Conv2d(256, 512, kernel_size=(3, 3))),
            ('relu9', nn.ReLU())
        ]))

        self.fc = nn.Sequential(OrderedDict([
            ('f10', nn.Linear(512, 128)),
            ('relu10', nn.ReLU()),
            ('f11', nn.Linear(128, 84)),
            ('relu11', nn.ReLU()),
            ('f12', nn.Linear(84, 30)), # inizialmente 10, ma noi abbiamo 30 "classi"
            #('sig12', nn.Softmax(dim=-1))
        ]))


    def forward(self, img):
        output = self.convnet(img.double())
        output = output.view(img.size(0), -1)
        output = self.fc(output)
        return output

class KeyPointsDataset(VisionDataset):
  
  def __init__(self, root, split="train", transform=None, target_transform=None):
      super(KeyPointsDataset, self).__init__(root, transform=transform, target_transform=target_transform)

      filename = "/training.csv" if split == "train" else "/test.csv"
      self.samples = make_dataset(root + filename)
      self.loader = pil_loader
  
  def __getitem__(self, index):

      image, label = self.samples[index] # Provide a way to access image and label via index
                       # Image should be a PIL Image
                      # label can be int
      #print("type of image before transform:", type(image))
      sample = self.loader(image)

      # Applies preprocessing when accessing the image
      if self.transform is not None:
          image = self.transform(sample)
      if self.target_transform is not None:
          label = self.target_transform(label)

      return image, label
  
  def __len__(self):
      return len(self.samples)

"""**NETWORK PARAMETERS INITIALIZATION**"""

DEVICE = 'cuda' # 'cuda' or 'cpu'

NUM_CLASSES = 30 # PACS has 7 classess 

BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing
                     # the batch size, learning rate should change by the same factor to have comparable results

LR = 1e-3            # The initial Learning Rate
MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD
WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default

NUM_EPOCHS = 30     # Total number of training epochs (iterations over dataset)
STEP_SIZE = 22     # How many epochs before decreasing learning rate (if using a step-down policy)
GAMMA = 0.1          # Multiplicative factor for learning rate step-down

LOG_FREQUENCY = 10

# Define transforms for training phase
train_transform = transforms.Compose([#transforms.Resize(32),
                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor
                                      #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes tensor with mean and standard deviation
])
# Define transforms for the evaluation phase
target_transform = transforms.Compose([transforms.ToTensor(),
                                    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    
])

net = LeNet5()
net = net.double()
# Define loss function
criterion = nn.MSELoss() # for classification, we use Cross Entropy

# Choose parameters to optimize
# To access a different set of parameters, you have to access submodules of AlexNet
# (nn.Module objects, like AlexNet, implement the Composite Pattern)
# e.g.: parameters of the fully connected layers: net.classifier.parameters()
# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) 
parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet

# Define optimizer
# An optimizer updates the weights based on loss
# We use SGD with momentum
optimizer = optim.Adam(parameters_to_optimize, lr=LR, weight_decay=WEIGHT_DECAY)

# Define scheduler
# A scheduler dynamically changes learning rate
# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)

"""**LOAD DATASETS**"""

DATA_DIR = '/content'
train_dataset = KeyPointsDataset(DATA_DIR, split="train", transform=train_transform)
train_indexes = [idx for idx in range(len(train_dataset)) if idx % 2]
val_indexes = [idx for idx in range(len(train_dataset)) if not idx % 2]
validation_dataset = Subset(train_dataset, val_indexes)
train_dataset = Subset(train_dataset, train_indexes)

"""**GRID SEARCH**"""

# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)
validate_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=True)

batches = [128, 256, 512]
lr_arr = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]
results = []

loss_history = []
count = 0
best_net = None
best_loss = math.inf

for bat in batches:
  BATCH_SIZE = bat
  for lr in lr_arr:
    LR = lr
    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)
    validate_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=True)
    net = LeNet5()
    net = net.double()
    net= net.to(DEVICE)
    cudnn.benchmark 
    parameters_to_optimize = net.parameters()
    optimizer = optim.Adam(parameters_to_optimize, lr=LR, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)
    train_loss_hist = []
    val_loss_hist = []
    for epoch in range(NUM_EPOCHS):
      print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))
      tr_loss = train(train_dataloader, net) 
      vd_loss = validate(validate_dataloader, net)
      train_loss_hist.append(tr_loss)
      val_loss_hist.append(vd_loss)
      if vd_loss < best_loss:
        best_loss = vd_loss
        best_net = copy.deepcopy(net)
      loss_history.append(tr_loss)
      scheduler.step() 
    result = ResultItem(NUM_EPOCHS, BATCH_SIZE, lr, train_loss_hist, val_loss_hist)
    results.append(result)

"""**PLOT GRID SEARCH RESULT**"""

# crea una figura, 1 Ã¨ il numero che diamo alla figura, figsize sono le dimensioni
best_res = None
best_loss = math.inf
for i in range(15):
  if i % 3 == 0:
    res1 = results[i]
    res2 = results[i + 1]
    res3 = results[i + 2]
    if min(res1.vd_loss) < best_loss:
      best_res = res1
      best_loss = min(res1.vd_loss)
    if min(res2.vd_loss) < best_loss:
      best_res = res2
      best_loss = min(res2.vd_loss)
    if min(res3.vd_loss) < best_loss:
      best_res = res3
      best_loss = min(res3.vd_loss)
    fig_1 = plt.figure(1, figsize=(20, 4.8))
    chart_1 = fig_1.add_subplot(131) # line plot for loss vs epochs (both train and validation)
    chart_2 = fig_1.add_subplot(132) # line plot for accuracy vs epochs (both train and validation)
    chart_3 = fig_1.add_subplot(133) # line plot for accuracy vs epochs (both train and validation)
    tr_ls_plt = chart_1.plot(range(NUM_EPOCHS), res1.tr_loss, label='training')
    vd_ls_plt = chart_1.plot(range(NUM_EPOCHS), res1.vd_loss, label='validation')
    tr_ac_plt = chart_2.plot(range(NUM_EPOCHS), res2.tr_loss, label="training")
    vd_ac_plt = chart_2.plot(range(NUM_EPOCHS), res2.vd_loss, label="validation")
    tr_ac_plt = chart_3.plot(range(NUM_EPOCHS), res3.tr_loss, label="training")
    vd_ac_plt = chart_3.plot(range(NUM_EPOCHS), res3.vd_loss, label="validation")
    chart_1.set_title("Loss vs Epochs LR:{}, BATCH_SIZE:{}".format(res1.lr, res1.batch))
    chart_1.legend()
    chart_1.set_ylabel("loss")
    chart_1.set_xlabel("epochs")
    chart_2.set_title("Loss vs Epochs LR:{}, BATCH_SIZE:{}".format(res2.lr, res2.batch))
    chart_2.legend()
    chart_2.set_ylabel("loss")
    chart_2.set_xlabel("epochs")
    chart_3.set_title("Loss vs Epochs LR:{}, BATCH_SIZE:{}".format(res3.lr, res3.batch))
    chart_3.legend()
    chart_3.set_ylabel("loss")
    chart_3.set_xlabel("epochs")
    plt.show()

# print max result
print("best_res: batch{}, lr:{}, min_loss:{}".format(best_res.batch, best_res.lr, min(best_res.vd_loss)))